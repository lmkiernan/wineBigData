# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nuO9iFuqp6Pg2sqCX5KNVyf62sgwkgPW

### **Chapter 1: Data Cleaning**
"""

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

drive.mount('/content/drive')
data_path = '/content/drive/My Drive/winemag-data-130k-v2.csv'
df = pd.read_csv(data_path)
print(df.head())

print(df.info())
print(df.isnull().sum())

print("Number of duplicate rows:", df.duplicated().sum())
df = df.dropna(subset=['points', 'price'])
print("Number of rows after removing nulls in 'points' and 'price':", df.shape[0])
# Calculate IQR for 'price' column
Q1 = df['price'].quantile(0.25)
Q3 = df['price'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outlier removal
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter rows within IQR
df_no_outliers = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]
print("Number of rows after removing outliers:", df_no_outliers.shape[0])

# Check distribution of 'price'
plt.figure(figsize=(10, 6))
sns.histplot(df['price'], bins=50, kde=True)
plt.title('Distribution of Wine Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
# Bucket 'points' into intervals of 5
df_no_outliers['points_bucket'] = pd.cut(
df_no_outliers['points'], bins=range(80, 101, 2), right=False
)

# Reverse the categories and convert to a list
reversed_categories = list(reversed(df_no_outliers['points_bucket'].cat.categories))

# Reorder the categories
df_no_outliers['points_bucket'] = df_no_outliers['points_bucket'].cat.reorder_categories(
    reversed_categories, ordered=True
)

# Plot again
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_no_outliers, x='price', y='points_bucket')
plt.title("Boxplot of Price by Points Buckets (Without Outliers)")
plt.xlabel("Price (in $)")
plt.ylabel("Points (Bucketed in 5)")
plt.show()

print(df_no_outliers)

"""The data cleaning and wrangling section began by importing libraries essential for analysis and connecting Google Drive to access the dataset. The initial cell loaded a CSV file containing wine data and displayed a preview, ensuring the file loaded correctly. Next, the info() function summarized the dataset’s structure, showing column names, data types, and null counts, while isnull().sum() identified missing values. Duplicates were checked, and rows with null values in points and price columns were dropped, reducing the dataset size to 120,975 rows.

To assess price distribution, a histogram visualized its skewness, revealing many wines in lower price ranges with outliers at the upper end. The next step applied the IQR method to define bounds for outlier removal in the price column. After filtering, a boxplot of price versus points showed the positive correlation between higher scores and prices, now cleaned of extreme values.

### **Chapter 2: Early Insights (Pre Check-In)**
"""

# Group the data by 'country' and calculate the mean of 'points'
avg_points_by_country = df.groupby('country')['points'].mean().sort_values(ascending=False)

# Filter to show the top 15 countries by average points
top_countries = avg_points_by_country.head(15)

# Plot the data
plt.figure(figsize=(12, 8))
sns.barplot(x=top_countries.values, y=top_countries.index, palette="viridis")
plt.title("Average Points by Country (Top 15)")
plt.xlabel("Average Points")
plt.ylabel("Country")
plt.xlim(85, top_countries.values.max() + 1)  # Ensure x-axis starts at 85
plt.show()

print(df)

# Group by variety and calculate average points, rounded to 2 decimal places
avg_points_by_variety = df.groupby('variety')['points'].mean().round(2).sort_values(ascending=False).head(15)

# Plot the data
plt.figure(figsize=(12, 8))
sns.barplot(x=avg_points_by_variety.values, y=avg_points_by_variety.index, palette="viridis")
plt.title("Top 15 Wine Varieties by Average Points")
plt.xlabel("Average Points")
plt.ylabel("Variety")
plt.xlim(85, avg_points_by_variety.values.max() + 1)  # Adjust x-axis
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Filter relevant columns
data = df[['price', 'points', 'country', 'variety']].dropna()

# Split features and target
X = data[['price', 'variety']]
y = data['points']

# Preprocessing
# Categorical columns: 'country', 'variety'
# Numerical column: 'price'
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['price']),
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['variety'])
    ]
)

# Model pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42, n_estimators=100))
])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Model Performance:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"R² Score: {r2:.2f}")

import plotly.express as px

country_stats = data.copy

# Group by Country and compute mean points, mean price, and wine count
country_stats = data.groupby('country').agg({
    'points': 'mean',
    'price': 'mean'
}).reset_index()

# Add total_wines by counting wines for each Country
country_stats['total_wines'] = data['country'].value_counts().reindex(country_stats['country']).values

# Prepare the dataset
country_stats['bubble_size'] = (country_stats['total_wines'] / 200) + 10  # Adjust bubble size

# Scatter_geo Plot
fig = px.scatter_geo(
    country_stats,
    locations='country',
    locationmode='country names',
    size='bubble_size',
    color='points',
    title='Wine Stats: Quantity (Bubble Size) & Average Points (Color)',
    color_continuous_scale='Viridis',
    projection='equirectangular',
    opacity=0.9
)

# Fix hover template with clean display
fig.update_traces(
    hovertemplate="<b>Country:</b> %{hovertext}<br>" +
                  "<b>Total Wines:</b> %{customdata[0]}<br>" +
                  "<b>Average Points:</b> %{customdata[1]:.2f}<extra></extra>",
    customdata=country_stats[['total_wines', 'points']].values,
    hovertext=country_stats['country']
)

# Update layout for clean visuals
fig.update_layout(
    geo=dict(
        showland=True, landcolor="rgb(243, 243, 243)",
        showcountries=True, countrycolor="rgb(204, 204, 204)"
    ),
    margin={"r": 0, "t": 50, "l": 0, "b": 0}
)

# Show the map
fig.show()

import re
def extract_keywords(text):
  pattern = r'\b(fruity|bold|earthy|tannins|sweet|acidic|dry|tart|soft|rich)\b'
  return ', '.join(re.findall(pattern, text.lower()))

df['keywords'] = df['description'].apply(lambda x: extract_keywords(str(x)))

print(df)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# Explode 'keywords' column into separate rows
keywords_exploded = df[['keywords', 'points']].copy()
keywords_exploded['keywords'] = keywords_exploded['keywords'].str.split(', ')
keywords_exploded = keywords_exploded.explode('keywords')

# One-Hot Encode keywords into features
keywords_encoded = pd.get_dummies(keywords_exploded['keywords'], prefix='keyword')
X = keywords_encoded  # Keyword features
y = keywords_exploded['points']  # Target: points

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and Evaluate
y_pred = model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

"""The first step groups the data by country and calculates the mean of wine points, identifying the top 15 countries with the highest average scores. A barplot visualizes these countries, starting the x-axis at 85 for clarity. This step highlights geographic trends in wine quality, with countries like England, India, and Austria emerging as top performers. This provides a regional perspective on wine ratings, setting the stage for further analysis at the varietal level.

The second step shifts focus to wine varieties, grouping data to calculate the average points for each variety and selecting the top 15 performers. By rounding the results and sorting them in descending order, the visualization reveals standout varieties such as Terrantez, Tinta del Pais, and Gelber Traminer. This analysis offers insight into varietal-specific trends, helping to identify which wine types are consistently associated with higher ratings. Together with the previous step, it provides a deeper understanding of both regional and varietal factors influencing wine quality.

The final steps prepare the data for predictive modeling and implement a Random Forest Regressor to predict wine points based on price and variety. Relevant columns are selected, and preprocessing is performed using a combination of scaling for numerical data and one-hot encoding for categorical features. The data is split into training and testing sets, and the model is evaluated using metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and R² score. With an R² of 0.41, the model captures some relationship between the features and wine scores but suggests further factors may influence quality.

### **Chapter 3: Merging Data**

One such influential factor may be the climate of the region during the grape-growing season. Grapevines are highly sensitive to environmental conditions, including temperature, rainfall, and seasonal variations, which impact grape ripening, sugar concentration, and overall flavor profiles. For example, regions with balanced heat and moderate rainfall often produce high-quality wines, whereas extreme temperatures or inconsistent precipitation can hinder grape development. This could help explain differences in wine scores between countries and varieties, as observed in our earlier analysis.

We suspect that variations in climate conditions across regions may play a significant role in driving the quality and perceived value of wines. Exploring this relationship could uncover deeper insights into why certain countries, such as England or Austria, achieve higher average points or why certain wine varieties excel under specific environmental conditions.

To investigate this hypothesis, we will incorporate a new dataset containing the weather data for 155 countries in 2020. By joining this dataset with our wine dataset, we can perform further EDA and develop linear regression models to examine the correlation between wine quality regional climate conditions, specifically average temperature and precipitation throughout the Spring season.
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
weather_path = '/content/drive/My Drive/the_weather_of_155_countries_in_2020.csv'
weather_df = pd.read_csv(weather_path)

print("Weather Dataset:")
print(weather_df.head())

weather_df.rename(columns={'Country/Region': 'Country'}, inplace=True)
df.rename(columns={'country': 'Country'}, inplace=True)
df['Country'] = df['Country'].str.strip().str.title()
weather_df['Country'] = weather_df['Country'].str.strip().str.title()
common_countries = set(df['Country']).intersection(set(weather_df['Country']))
print(f"Number of common countries: {len(common_countries)}")

weather_agg = weather_df.groupby('Country').agg({
    'TAVG': 'mean',  # Average temperature
    'PRCP': 'sum',   # Total precipitation
}).reset_index()

weather_agg.rename(columns={
    'TAVG': 'Avg_Temperature',
    'PRCP': 'Total_Precipitation',
}, inplace=True)

print(weather_agg.head())

print(df.dtypes)
print(weather_agg.dtypes)

import sqlite3
import pandas as pd

# Create a connection to an in-memory SQLite database
conn = sqlite3.connect(':memory:')

# Load the wine and weather data into SQLite tables
df.to_sql('wine', conn, index=False, if_exists='replace')
weather_agg.to_sql('weather', conn, index=False, if_exists='replace')

# Perform the inner join using SQL
query = """
SELECT
    w.id,
    w.Country,
    w.description,
    w.designation,
    w.points,
    w.price,
    w.province,
    w.region_1,
    w.region_2,
    w.taster_name,
    w.taster_twitter_handle,
    w.title,
    w.variety,
    w.winery,
    wa.Avg_Temperature,
    wa.Total_Precipitation
FROM wine w
INNER JOIN weather wa
ON w.Country = wa.Country;
"""

# Execute the query and fetch the result as a DataFrame
combined_df = pd.read_sql_query(query, conn)

# Save the combined dataset to a CSV file
combined_df.to_csv('/content/drive/My Drive/combined_wine_weather_aggregated_sql.csv', index=False)

# Display the combined DataFrame
print("Combined Dataset (Wine + Weather Aggregated):")
print(combined_df.head())

# Close the database connection
conn.close()

print(f"Rows in Combined Dataset: {combined_df.shape[0]}")
print(f"Columns in Combined Dataset: {combined_df.shape[1]}")
print(combined_df.info())

"""In this chapter, we merged two datasets—wine reviews and weather data—to investigate the relationship between regional climate conditions and wine quality. Initially, the datasets were loaded, and country names were standardized to identify 34 overlapping countries. The weather data was aggregated to calculate average temperature and total precipitation by country. Using SQL, an inner join operation was performed to combine the wine and weather data into a unified dataset. The resulting dataset, containing 16 columns and over 120,000 rows, was validated to ensure successful merging. This combined dataset will enable further analysis of how climate factors influence wine scores.

### **Chapter 4: Insights From Merge**

Analyzing the Relationship Between Average Temperature and Wine Points
"""

plt.figure(figsize=(12, 6))
sns.regplot(data=combined_df, x='Avg_Temperature', y='points', scatter_kws={'alpha': 0.6}, line_kws={'color': 'red'})
plt.title('Trend Between Average Temperature and Wine Points')
plt.xlabel('Average Temperature (°C)')
plt.ylabel('Wine Points')
plt.grid(alpha=0.5)
plt.show()

temp_bins = pd.cut(combined_df['Avg_Temperature'], bins=5)
combined_df['Temp_Bin'] = temp_bins
temp_points_heatmap = combined_df.groupby('Temp_Bin', as_index=False)['points'].mean()
bin_labels = [f"{interval.left:.1f} - {interval.right:.1f}°C" for interval in temp_bins.cat.categories]

plt.figure(figsize=(12, 8))
sns.barplot(
    x=bin_labels, y=temp_points_heatmap['points'], palette='coolwarm'
)
plt.title('Average Wine Points by Temperature Range')
plt.xlabel('Average Temperature (°C)')
plt.ylabel('Average Wine Points')
plt.xticks(rotation=45)  # Rotate labels for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""The analysis indicates a slight negative trend between average temperatures and wine points, with higher temperatures correlating with lower average wine ratings. This suggests that regions with moderate temperatures might produce wines that receive higher scores.

EDA 2: Analyzing the Relationship Between Average Precipitation and Wine Points
"""

# Scatterplot with regression line for precipitation and wine points
plt.figure(figsize=(12, 6))
sns.regplot(
    data=combined_df, x='Total_Precipitation', y='points',
    scatter_kws={'alpha': 0.6}, line_kws={'color': 'red'}
)
plt.title('Trend Between Total Precipitation and Wine Points')
plt.xlabel('Total Precipitation (mm)')
plt.ylabel('Wine Points')
plt.grid(alpha=0.5)
plt.show()

precip_filtered_df = combined_df[combined_df['Total_Precipitation'] <= 50000]

print("Descriptive statistics for wine points:")
print(combined_df['points'].describe())

plt.figure(figsize=(12, 6))
sns.regplot(
    data=precip_filtered_df, x='Total_Precipitation', y='points',
    scatter_kws={'alpha': 0.6}, line_kws={'color': 'red'}
)
plt.title('Trend Between Total Precipitation and Wine Points (All Scores)')
plt.xlabel('Total Precipitation (mm)')
plt.ylabel('Wine Points')
plt.grid(alpha=0.5)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Select the features (X) and target (y)
X = combined_df[['Avg_Temperature']]  # Feature: Average Temperature
y = combined_df['points']  # Target: Wine Points

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the results
print("Linear Regression Results:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.2f}")

combined_df.head(10)

"""This section systematically explored the relationship between weather variables, specifically average temperature and total precipitation, and wine quality as measured by wine points. Initially, a scatterplot analysis examined the trend between average temperature and wine scores, revealing a slight negative relationship where higher temperatures aligned with lower wine points. To deepen this analysis, the temperature data was grouped into bins, and average scores were compared across these temperature ranges, showing that moderate temperatures tended to produce higher-quality wines. A similar approach was applied to total precipitation, revealing a weak positive relationship with wine scores. To refine the insights, outlier precipitation values were filtered, and descriptive statistics for wine points were calculated. Finally, a linear regression model was constructed using average temperature as the predictor and wine points as the target. Despite the suspected influence of weather, the results showed weak predictive power, as indicated by a low R² score of 0.01 and a high mean squared error (9.26).

### **Chapter 5: Feature Importance and Modelling**
"""

# Import necessary libraries
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import numpy as np

# Step 1: Encode the target variable 'points_category'
combined_df['points_category'] = pd.cut(
    combined_df['points'], bins=[0, 85, 90, 100], labels=['Low', 'Medium', 'High']
)

# Step 2: Encode the categorical features using LabelEncoder
le_winery = LabelEncoder()
le_variety = LabelEncoder()

combined_df['winery_label'] = le_winery.fit_transform(combined_df['winery'])
combined_df['variety_label'] = le_variety.fit_transform(combined_df['variety'])

# Step 3: Define the features (X) and target (y)
X = combined_df[['winery_label', 'variety_label', 'price']]
y = combined_df['points_category']

# Drop NaN rows to avoid errors
X = X.dropna()
y = y.dropna()

# Ensure alignment between X and y
X = X.loc[y.index]

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Encode the target variable for training
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Step 5: RandomizedSearchCV setup
param_distributions = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(),
    param_distributions=param_distributions,
    n_iter=20,
    scoring='accuracy',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Step 6: Fit the model
random_search.fit(X_train, y_train_encoded)

# Step 7: Best Parameters and Evaluation
print("Best Parameters:", random_search.best_params_)
print("Best Score:", random_search.best_score_)

# Use the best estimator to predict on the test set
best_rf = random_search.best_estimator_
y_pred = best_rf.predict(X_test)

# Decode predictions back to original labels
y_pred_labels = le.inverse_transform(y_pred)

# Step 8: Accuracy and Classification Report
print("Accuracy:", accuracy_score(y_test_encoded, y_pred))
print("Classification Report:\n", classification_report(y_test_encoded, y_pred))

print(y)

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Encode the categorical target variable
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Define the model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# Hyperparameter grid
param_distributions = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [3, 5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_distributions,
    n_iter=20,
    scoring='r2',
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Fit the model
random_search.fit(X_train, y_train_encoded)

# Best parameters and model
best_xgb = random_search.best_estimator_
print("Best Parameters:", random_search.best_params_)

# Evaluate on test set
y_pred = best_xgb.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test_encoded, y_pred))
r2 = r2_score(y_test_encoded, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

from sklearn.preprocessing import LabelEncoder

data = combined_df.copy()
# Step 3: Preprocess data
# Fill missing values and scale numeric climate features
data['Avg_Temperature'].fillna(data['Avg_Temperature'].median(), inplace=True)
data['Total_Precipitation'].fillna(data['Total_Precipitation'].median(), inplace=True)
label_encoder = LabelEncoder()
data['province'] = label_encoder.fit_transform(data['province'].astype(str))
data['variety'] = label_encoder.fit_transform(data['variety'].astype(str))
data['winery'] = label_encoder.fit_transform(data['winery'].astype(str))
scaler = StandardScaler()
data[['temperature', 'precipitation']] = scaler.fit_transform(data[['Avg_Temperature', 'Total_Precipitation']])

# Step 4: Train a Random Forest for feature importance
features = ['price', 'variety', 'temperature', 'precipitation', 'province', 'winery']  # Add other features if needed
X = data[features]
y = data['points']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Step 5: Visualize Feature Importance
importances = rf.feature_importances_
feature_names = X.columns

# Plot feature importance
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
fig = px.bar(importance_df, x='Importance', y='Feature', title='Feature Importance', orientation='h')
fig.show()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Label Encode categorical features
le_winery = LabelEncoder()
le_variety = LabelEncoder()
log_reg_df = combined_df.copy()
log_reg_df['winery_label'] = le_winery.fit_transform(log_reg_df['winery'])
log_reg_df['variety_label'] = le_variety.fit_transform(log_reg_df['variety'])
log_reg_df['points_category'] = pd.cut(data['points'], bins=[0, 85, 90, 100], labels=['Low', 'Medium', 'High'])

# Select features and target
X = log_reg_df[['winery_label', 'variety_label', 'price']]  # No one-hot encoding
y = log_reg_df['points_category']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Predictions and Evaluation
y_pred = log_reg.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=log_reg.classes_)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()